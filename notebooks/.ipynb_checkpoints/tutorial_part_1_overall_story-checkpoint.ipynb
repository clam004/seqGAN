{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c246fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from models import generator\n",
    "from models import discriminator\n",
    "from trainers import train_generator_MLE, train_generator_PG, train_discriminator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ccf98",
   "metadata": {},
   "source": [
    "# Tutorial Part 1 Overall Story\n",
    "\n",
    "here we will describe each step of the pipeline at a high level to help us contextualize our future learnings\n",
    "\n",
    "\n",
    "## Synthetic Data Experiment\n",
    "\n",
    "The most accurate way of evaluating generative models is that we draw some samples from it and let human observers re- view them based on their prior knowledge. We assume that the human observer has learned an accurate model of the natural distribution p_human(x). \n",
    " \n",
    "the authots used a randomly initialized language model as the true model, aka, the ***oracle***, to generate the \"real\" data distribution p(x_t |x_1 , . . . , x_t−1 ). The benefit of having such oracle is that firstly, it provides the training dataset and secondly evaluates the exact perfor- mance of the generative models, which will not be possible with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674f4b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental constants\n",
    "\n",
    "CUDA = False\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_SEQ_LEN = 20\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "START_LETTER = 0\n",
    "\n",
    "GEN_EMBEDDING_DIM = 32 # length of input vectors for generator and oracle\n",
    "GEN_HIDDEN_DIM = 32 # length of hidden state for generator and oracle\n",
    "\n",
    "oracle_state_dict_path = '../oracle_EMBDIM32_HIDDENDIM32_VOCAB5000_MAXSEQLEN20.trc'\n",
    "oracle_samples_path = '../oracle_samples.trc'\n",
    "\n",
    "\n",
    "MLE_TRAIN_EPOCHS = 100\n",
    "ADV_TRAIN_EPOCHS = 50\n",
    "POS_NEG_SAMPLES = 10000\n",
    "\n",
    "DIS_EMBEDDING_DIM = 64 # length of input vectors for discriminator\n",
    "DIS_HIDDEN_DIM = 64 # length of hidden state for discriminator\n",
    "\n",
    "pretrained_gen_path = '../gen_MLEtrain_EMBDIM32_HIDDENDIM32_VOCAB5000_MAXSEQLEN20.trc'\n",
    "pretrained_dis_path = '../dis_pretrain_EMBDIM_64_HIDDENDIM64_VOCAB5000_MAXSEQLEN20.trc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6c4c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (embeddings): Embedding(5000, 32)\n",
       "  (gru): GRU(32, 32)\n",
       "  (gru2out): Linear(in_features=32, out_features=5000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle = generator.Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "\n",
    "# for reproducibiility we provide saved parameters for the oracle\n",
    "oracle.load_state_dict(torch.load(oracle_state_dict_path))\n",
    "\n",
    "oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537abed",
   "metadata": {},
   "source": [
    "The output above should look like this\n",
    "\n",
    "```\n",
    "Generator(\n",
    "  (embeddings): Embedding(5000, 32)\n",
    "  (gru): GRU(32, 32)\n",
    "  (gru2out): Linear(in_features=32, out_features=5000, bias=True)\n",
    ")\n",
    "```\n",
    "\n",
    "To explain the information above: the model has 5000 possible input vectors in its vocab each with length 32, the GRU takes vectors  length 32 and outputs activation of the same length. The output returns activations of the same length as the vocab.\n",
    "\n",
    "the authors use the oracle to generate 10,000 sequences of length 20 as the training set S for the generative models.\n",
    "we have already used helpers.batchwise_sample() to save S so you can load it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b247f28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([10000, 20])\n"
     ]
    }
   ],
   "source": [
    "oracle_samples = torch.load(oracle_samples_path).type(torch.LongTensor)\n",
    "print(type(oracle_samples), oracle_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a401d2f",
   "metadata": {},
   "source": [
    "### instantiate a generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef7151c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generator.Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "dis = discriminator.Discriminator(DIS_EMBEDDING_DIM, DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "311f7a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (embeddings): Embedding(5000, 32)\n",
       "  (gru): GRU(32, 32)\n",
       "  (gru2out): Linear(in_features=32, out_features=5000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44063b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (embeddings): Embedding(5000, 64)\n",
       "  (gru): GRU(64, 64, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "  (gru2hidden): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (dropout_linear): Dropout(p=0.2, inplace=False)\n",
       "  (hidden2out): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4b90b",
   "metadata": {},
   "source": [
    "### If you have and want to use GPU, all models and model inputs need to be on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cd7929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "    oracle = oracle.cuda()\n",
    "    gen = gen.cuda()\n",
    "    dis = dis.cuda()\n",
    "    oracle_samples = oracle_samples.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c3ccd",
   "metadata": {},
   "source": [
    "### GENERATOR MLE TRAINING\n",
    "\n",
    "At the beginning of the training, the authors used maximum likelihood estimation (MLE) to pretrain Gθ on training set S. \n",
    "\n",
    "They found the supervised signal from the pretrained discriminator is informative to help adjust the generator efficiently.\n",
    "\n",
    "```\n",
    "# GENERATOR MLE TRAINING\n",
    "print('Starting Generator MLE Training...')\n",
    "gen_optimizer = optim.Adam(gen.parameters(), lr=1e-2)\n",
    "train_generator_MLE(gen, gen_optimizer, oracle, oracle_samples, MLE_TRAIN_EPOCHS)\n",
    "torch.save(gen.state_dict(), pretrained_gen_path)\n",
    "\n",
    "# PRETRAIN DISCRIMINATOR\n",
    "print('Starting Discriminator Training...')\n",
    "dis_optimizer = optim.Adagrad(dis.parameters())\n",
    "train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, d_steps = 50,  epochs = 3)\n",
    "torch.save(dis.state_dict(), pretrained_dis_path)\n",
    "```\n",
    "\n",
    "The below pretraining pretraining only needs to be done once, once saved it can be loaded using `model.load_state_dict(torch.load(pretrained_gen_path))` two cells down while skipping th enet two cells\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9097bf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Generator MLE Training...\n"
     ]
    }
   ],
   "source": [
    "# GENERATOR MLE TRAINING\n",
    "print('Starting Generator MLE Training...')\n",
    "gen_optimizer = optim.Adam(gen.parameters(), lr=1e-2)\n",
    "train_generator_MLE(gen, gen_optimizer, oracle, oracle_samples, MLE_TRAIN_EPOCHS)\n",
    "torch.save(gen.state_dict(), pretrained_gen_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a83970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Discriminator Training...\n"
     ]
    }
   ],
   "source": [
    "# PRETRAIN DISCRIMINATOR\n",
    "print('Starting Discriminator Training...')\n",
    "dis_optimizer = optim.Adagrad(dis.parameters())\n",
    "train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, d_steps = 50,  epochs = 3)\n",
    "torch.save(dis.state_dict(), pretrained_dis_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3fdf48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained generator and discrimnator\n",
    "gen.load_state_dict(torch.load(pretrained_gen_path))\n",
    "dis.load_state_dict(torch.load(pretrained_dis_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059af8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
