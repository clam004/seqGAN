{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e41a2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from models import generator\n",
    "from models import discriminator\n",
    "from trainers import train_generator_MLE, train_generator_PG, train_discriminator\n",
    "import helpers\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885703f8",
   "metadata": {},
   "source": [
    "# Tutorial Part 1 Overall Story\n",
    "\n",
    "here we will describe each step of the pipeline at a high level to help us contextualize our future learnings\n",
    "\n",
    "\n",
    "## Synthetic Data Experiment\n",
    "\n",
    "The most accurate way of evaluating generative models is that we draw some samples from it and let human observers re- view them based on their prior knowledge. We assume that the human observer has learned an accurate model of the natural distribution p_human(x). \n",
    " \n",
    "the authots used a randomly initialized language model as the true model, aka, the ***oracle***, to generate the \"real\" data distribution p(x_t |x_1 , . . . , x_t−1 ). The benefit of having such oracle is that firstly, it provides the training dataset and secondly evaluates the exact perfor- mance of the generative models, which will not be possible with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d109bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental constants\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_SEQ_LEN = 20\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "START_LETTER = 0\n",
    "\n",
    "GEN_EMBEDDING_DIM = 32 # length of input vectors for generator and oracle\n",
    "GEN_HIDDEN_DIM = 32 # length of hidden state for generator and oracle\n",
    "\n",
    "oracle_state_dict_path = '../params/oracle_EMBDIM32_HIDDENDIM32_VOCAB5000_MAXSEQLEN20.trc'\n",
    "oracle_samples_path = '../sample_data/oracle_samples.trc'\n",
    "\n",
    "MLE_TRAIN_EPOCHS = 100\n",
    "POS_NEG_SAMPLES = 10000\n",
    "\n",
    "DIS_EMBEDDING_DIM = 64 # length of input vectors for discriminator\n",
    "DIS_HIDDEN_DIM = 64 # length of hidden state for discriminator\n",
    "\n",
    "pretrained_gen_path = '../params/gen_MLEtrain_EMBDIM32_HIDDENDIM32_VOCAB5000_MAXSEQLEN20.trc'\n",
    "pretrained_dis_path = '../params/dis_pretrain_EMBDIM_64_HIDDENDIM64_VOCAB5000_MAXSEQLEN20.trc'\n",
    "\n",
    "pretrained_gen_path_cpu = '../params/gen_MLEtrain_EMBDIM32_HIDDENDIM32_VOCAB5000_MAXSEQLEN20_cpu.trc'\n",
    "pretrained_dis_path_cpu = '../params/dis_pretrain_EMBDIM_64_HIDDENDIM64_VOCAB5000_MAXSEQLEN20_cpu.trc'\n",
    "\n",
    "ADV_TRAIN_EPOCHS = 50 # ADVERSARIAL TRAINING EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572ec55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (embeddings): Embedding(5000, 32)\n",
       "  (gru): GRU(32, 32)\n",
       "  (gru2out): Linear(in_features=32, out_features=5000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle = generator.Generator(\n",
    "    GEN_EMBEDDING_DIM, \n",
    "    GEN_HIDDEN_DIM, \n",
    "    VOCAB_SIZE, \n",
    "    MAX_SEQ_LEN, \n",
    "    gpu=CUDA\n",
    ")\n",
    "\n",
    "# for reproducibiility we provide saved parameters for the oracle\n",
    "oracle.load_state_dict(torch.load(oracle_state_dict_path))\n",
    "\n",
    "oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7bd54",
   "metadata": {},
   "source": [
    "The output above should look like this\n",
    "\n",
    "```\n",
    "Generator(\n",
    "  (embeddings): Embedding(5000, 32)\n",
    "  (gru): GRU(32, 32)\n",
    "  (gru2out): Linear(in_features=32, out_features=5000, bias=True)\n",
    ")\n",
    "```\n",
    "\n",
    "To explain the information above: the model has 5000 possible input vectors in its vocab each with length 32, the GRU takes vectors  length 32 and outputs activation of the same length. The output returns activations of the same length as the vocab.\n",
    "\n",
    "the authors use the oracle to generate 10,000 sequences of length 20 as the training set S for the generative models.\n",
    "we have already used helpers.batchwise_sample() to save S so you can load it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4715013d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([10000, 20])\n"
     ]
    }
   ],
   "source": [
    "oracle_samples = torch.load(oracle_samples_path).type(torch.LongTensor)\n",
    "print(type(oracle_samples), oracle_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97b2571",
   "metadata": {},
   "source": [
    "### instantiate a generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d93c36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = optim.Adam(gen.parameters(), lr=1e-2)\n",
    "dis_optimizer = optim.Adagrad(dis.parameters())\n",
    "\n",
    "gen = generator.Generator(\n",
    "    GEN_EMBEDDING_DIM, \n",
    "    GEN_HIDDEN_DIM, \n",
    "    VOCAB_SIZE, \n",
    "    MAX_SEQ_LEN, \n",
    "    gpu=CUDA,\n",
    ")\n",
    "\n",
    "dis = discriminator.Discriminator(\n",
    "    DIS_EMBEDDING_DIM, \n",
    "    DIS_HIDDEN_DIM, \n",
    "    VOCAB_SIZE,\n",
    "    MAX_SEQ_LEN, \n",
    "    gpu=CUDA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c488af86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (embeddings): Embedding(5000, 32)\n",
       "  (gru): GRU(32, 32)\n",
       "  (gru2out): Linear(in_features=32, out_features=5000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d8f237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (embeddings): Embedding(5000, 64)\n",
       "  (gru): GRU(64, 64, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "  (gru2hidden): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (dropout_linear): Dropout(p=0.2, inplace=False)\n",
       "  (hidden2out): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc76ff",
   "metadata": {},
   "source": [
    "#### If you have and want to use GPU, all models and model inputs need to be on GPU\n",
    "\n",
    "If there is any mismatch between the parameters being on one device and the inputs being on another, then problems will arise. \n",
    "\n",
    "We run one call to the helpers.batchwise_oracle_nll function to test that we have the inputs and params on mathing devices, to get the baseline oracle NLL and to get a sense of the GPU/CPU speedup. On CPU the original batchwise_oracle_nll has a wall time of 22 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7de81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "    oracle = oracle.cuda()\n",
    "    gen = gen.cuda()\n",
    "    dis = dis.cuda()\n",
    "    oracle_samples = oracle_samples.cuda()\n",
    "    \n",
    "    print(torch.cuda.device_count(), torch.cuda.get_device_name(0))\n",
    "    \n",
    "    print(\n",
    "        next(gen.embeddings.parameters()).device, \n",
    "        next(dis.embeddings.parameters()).device,\n",
    "        next(oracle.embeddings.parameters()).device,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb298fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# sample from generator and compute baseline oracle NLL\n",
    "oracle_loss = helpers.batchwise_oracle_nll(\n",
    "    gen, \n",
    "    oracle, \n",
    "    POS_NEG_SAMPLES, \n",
    "    BATCH_SIZE, MAX_SEQ_LEN,\n",
    "    start_letter=START_LETTER, \n",
    "    gpu=CUDA,\n",
    ")\n",
    "\n",
    "print('oracle_loss', oracle_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf5a57",
   "metadata": {},
   "source": [
    "### GENERATOR MLE TRAINING\n",
    "\n",
    "At the beginning of the training, the authors used maximum likelihood estimation (MLE) to pretrain Gθ on training set S. \n",
    "\n",
    "They found the supervised signal from the pretrained discriminator is informative to help adjust the generator efficiently.\n",
    "\n",
    "```\n",
    "# GENERATOR MLE TRAINING\n",
    "print('Starting Generator MLE Training...')\n",
    "gen_optimizer = optim.Adam(gen.parameters(), lr=1e-2)\n",
    "train_generator_MLE(gen, gen_optimizer, oracle, oracle_samples, MLE_TRAIN_EPOCHS)\n",
    "torch.save(gen.state_dict(), pretrained_gen_path)\n",
    "\n",
    "# PRETRAIN DISCRIMINATOR\n",
    "print('Starting Discriminator Training...')\n",
    "dis_optimizer = optim.Adagrad(dis.parameters())\n",
    "train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, d_steps = 50,  epochs = 3)\n",
    "torch.save(dis.state_dict(), pretrained_dis_path)\n",
    "```\n",
    "\n",
    "The below pretraining only needs to be done once. After it is saved, it can be loaded using `model.load_state_dict(torch.load(pretrained_gen_path))` in the cell two cells down while skipping the next two cells. You can skip to ***Load Pretrained Generator and Discriminator***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2de2c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Generator MLE Training...\n",
      "epoch 1 : .......... average_train_NLL = 6.8282, oracle_sample_NLL = 14.6160\n",
      "epoch 2 : .......... average_train_NLL = 6.1780, oracle_sample_NLL = 13.7436\n",
      "epoch 3 : .......... average_train_NLL = 5.8610, oracle_sample_NLL = 13.1479\n",
      "epoch 4 : .......... average_train_NLL = 5.6560, oracle_sample_NLL = 12.8644\n",
      "epoch 5 : .......... average_train_NLL = 5.5087, oracle_sample_NLL = 12.5425\n",
      "epoch 6 : .......... average_train_NLL = 5.3974, oracle_sample_NLL = 12.3266\n",
      "epoch 7 : .......... average_train_NLL = 5.3087, oracle_sample_NLL = 12.1899\n",
      "epoch 8 : .......... average_train_NLL = 5.2363, oracle_sample_NLL = 12.0750\n",
      "epoch 9 : .......... average_train_NLL = 5.1747, oracle_sample_NLL = 11.9395\n",
      "epoch 10 : .......... average_train_NLL = 5.1223, oracle_sample_NLL = 11.8729\n",
      "epoch 11 : .......... average_train_NLL = 5.0776, oracle_sample_NLL = 11.8209\n",
      "epoch 12 : .......... average_train_NLL = 5.0383, oracle_sample_NLL = 11.7077\n",
      "epoch 13 : .......... average_train_NLL = 5.0031, oracle_sample_NLL = 11.6678\n",
      "epoch 14 : .......... average_train_NLL = 4.9722, oracle_sample_NLL = 11.5674\n",
      "epoch 15 : .......... average_train_NLL = 4.9448, oracle_sample_NLL = 11.5367\n",
      "epoch 16 : .......... average_train_NLL = 4.9199, oracle_sample_NLL = 11.4842\n",
      "epoch 17 : .......... average_train_NLL = 4.8971, oracle_sample_NLL = 11.5041\n",
      "epoch 18 : .......... average_train_NLL = 4.8772, oracle_sample_NLL = 11.4346\n",
      "epoch 19 : .......... average_train_NLL = 4.8570, oracle_sample_NLL = 11.4180\n",
      "epoch 20 : .......... average_train_NLL = 4.8420, oracle_sample_NLL = 11.3818\n",
      "epoch 21 : .......... average_train_NLL = 4.8253, oracle_sample_NLL = 11.3583\n",
      "epoch 22 : .......... average_train_NLL = 4.8114, oracle_sample_NLL = 11.3381\n",
      "epoch 23 : .......... average_train_NLL = 4.7979, oracle_sample_NLL = 11.2928\n",
      "epoch 24 : .......... average_train_NLL = 4.7840, oracle_sample_NLL = 11.2967\n",
      "epoch 25 : .......... average_train_NLL = 4.7722, oracle_sample_NLL = 11.2697\n",
      "epoch 26 : .......... average_train_NLL = 4.7633, oracle_sample_NLL = 11.2715\n",
      "epoch 27 : .......... average_train_NLL = 4.7525, oracle_sample_NLL = 11.2292\n",
      "epoch 28 : .......... average_train_NLL = 4.7441, oracle_sample_NLL = 11.2136\n",
      "epoch 29 : .......... average_train_NLL = 4.7346, oracle_sample_NLL = 11.2128\n",
      "epoch 30 : .......... average_train_NLL = 4.7258, oracle_sample_NLL = 11.1949\n",
      "epoch 31 : .......... average_train_NLL = 4.7178, oracle_sample_NLL = 11.1940\n",
      "epoch 32 : .......... average_train_NLL = 4.7085, oracle_sample_NLL = 11.1875\n",
      "epoch 33 : .......... average_train_NLL = 4.7041, oracle_sample_NLL = 11.1756\n",
      "epoch 34 : .......... average_train_NLL = 4.6953, oracle_sample_NLL = 11.1604\n",
      "epoch 35 : .......... average_train_NLL = 4.6908, oracle_sample_NLL = 11.1249\n",
      "epoch 36 : .......... average_train_NLL = 4.6830, oracle_sample_NLL = 11.1920\n",
      "epoch 37 : .......... average_train_NLL = 4.6793, oracle_sample_NLL = 11.1615\n",
      "epoch 38 : .......... average_train_NLL = 4.6713, oracle_sample_NLL = 11.1487\n",
      "epoch 39 : .......... average_train_NLL = 4.6641, oracle_sample_NLL = 11.0967\n",
      "epoch 40 : .......... average_train_NLL = 4.6587, oracle_sample_NLL = 11.1435\n",
      "epoch 41 : .......... average_train_NLL = 4.6581, oracle_sample_NLL = 11.1504\n",
      "epoch 42 : .......... average_train_NLL = 4.6557, oracle_sample_NLL = 11.0924\n",
      "epoch 43 : .......... average_train_NLL = 4.6442, oracle_sample_NLL = 11.0939\n",
      "epoch 44 : .......... average_train_NLL = 4.6413, oracle_sample_NLL = 11.0904\n",
      "epoch 45 : .......... average_train_NLL = 4.6350, oracle_sample_NLL = 11.1227\n",
      "epoch 46 : .......... average_train_NLL = 4.6360, oracle_sample_NLL = 11.0930\n",
      "epoch 47 : .......... average_train_NLL = 4.6324, oracle_sample_NLL = 11.0728\n",
      "epoch 48 : .......... average_train_NLL = 4.6271, oracle_sample_NLL = 11.0899\n",
      "epoch 49 : .......... average_train_NLL = 4.6214, oracle_sample_NLL = 11.0461\n",
      "epoch 50 : .......... average_train_NLL = 4.6182, oracle_sample_NLL = 11.0488\n",
      "epoch 51 : .......... average_train_NLL = 4.6208, oracle_sample_NLL = 11.1194\n",
      "epoch 52 : .......... average_train_NLL = 4.6134, oracle_sample_NLL = 11.0896\n",
      "epoch 53 : .......... average_train_NLL = 4.6083, oracle_sample_NLL = 11.0824\n",
      "epoch 54 : .......... average_train_NLL = 4.6047, oracle_sample_NLL = 11.0207\n",
      "epoch 55 : .......... average_train_NLL = 4.6029, oracle_sample_NLL = 11.0760\n",
      "epoch 56 : .......... average_train_NLL = 4.5962, oracle_sample_NLL = 11.0465\n",
      "epoch 57 : .......... average_train_NLL = 4.5984, oracle_sample_NLL = 11.0643\n",
      "epoch 58 : .......... average_train_NLL = 4.5932, oracle_sample_NLL = 11.0679\n",
      "epoch 59 : .......... average_train_NLL = 4.5862, oracle_sample_NLL = 11.0694\n",
      "epoch 60 : .......... average_train_NLL = 4.5828, oracle_sample_NLL = 11.0863\n",
      "epoch 61 : .......... average_train_NLL = 4.5823, oracle_sample_NLL = 11.0437\n",
      "epoch 62 : .......... average_train_NLL = 4.5846, oracle_sample_NLL = 11.0391\n",
      "epoch 63 : .......... average_train_NLL = 4.5784, oracle_sample_NLL = 11.0437\n",
      "epoch 64 : .......... average_train_NLL = 4.5776, oracle_sample_NLL = 11.0643\n",
      "epoch 65 : .......... average_train_NLL = 4.5773, oracle_sample_NLL = 11.0481\n",
      "epoch 66 : .......... average_train_NLL = 4.5728, oracle_sample_NLL = 11.0131\n",
      "epoch 67 : .......... average_train_NLL = 4.5712, oracle_sample_NLL = 11.0715\n",
      "epoch 68 : .......... average_train_NLL = 4.5695, oracle_sample_NLL = 11.0328\n",
      "epoch 69 : .......... average_train_NLL = 4.5669, oracle_sample_NLL = 11.0216\n",
      "epoch 70 : .......... average_train_NLL = 4.5602, oracle_sample_NLL = 11.0316\n",
      "epoch 71 : .......... average_train_NLL = 4.5619, oracle_sample_NLL = 10.9854\n",
      "epoch 72 : .......... average_train_NLL = 4.5612, oracle_sample_NLL = 11.0086\n",
      "epoch 73 : .......... average_train_NLL = 4.5794, oracle_sample_NLL = 11.0488\n",
      "epoch 74 : .......... average_train_NLL = 4.5559, oracle_sample_NLL = 10.9845\n",
      "epoch 75 : .......... average_train_NLL = 4.5568, oracle_sample_NLL = 10.9918\n",
      "epoch 76 : .......... average_train_NLL = 4.5527, oracle_sample_NLL = 10.9917\n",
      "epoch 77 : .......... average_train_NLL = 4.5528, oracle_sample_NLL = 10.9918\n",
      "epoch 78 : .......... average_train_NLL = 4.5539, oracle_sample_NLL = 10.9771\n",
      "epoch 79 : .......... average_train_NLL = 4.5429, oracle_sample_NLL = 10.9805\n",
      "epoch 80 : .......... average_train_NLL = 4.5412, oracle_sample_NLL = 10.9624\n",
      "epoch 81 : .......... average_train_NLL = 4.5406, oracle_sample_NLL = 10.9655\n",
      "epoch 82 : .......... average_train_NLL = 4.5381, oracle_sample_NLL = 10.9985\n",
      "epoch 83 : .......... average_train_NLL = 4.5443, oracle_sample_NLL = 10.9729\n",
      "epoch 84 : .......... average_train_NLL = 4.5395, oracle_sample_NLL = 10.9439\n",
      "epoch 85 : .......... average_train_NLL = 4.5384, oracle_sample_NLL = 10.9812\n",
      "epoch 86 : .......... average_train_NLL = 4.5381, oracle_sample_NLL = 10.9694\n",
      "epoch 87 : .......... average_train_NLL = 4.5347, oracle_sample_NLL = 10.9605\n",
      "epoch 88 : .......... average_train_NLL = 4.5463, oracle_sample_NLL = 10.9484\n",
      "epoch 89 : .......... average_train_NLL = 4.5331, oracle_sample_NLL = 10.9364\n",
      "epoch 90 : .......... average_train_NLL = 4.5292, oracle_sample_NLL = 10.9737\n",
      "epoch 91 : .......... average_train_NLL = 4.5300, oracle_sample_NLL = 10.9442\n",
      "epoch 92 : .......... average_train_NLL = 4.5254, oracle_sample_NLL = 10.9750\n",
      "epoch 93 : .......... average_train_NLL = 4.5268, oracle_sample_NLL = 10.9237\n",
      "epoch 94 : .......... average_train_NLL = 4.5202, oracle_sample_NLL = 10.9857\n",
      "epoch 95 : .......... average_train_NLL = 4.5195, oracle_sample_NLL = 10.9686\n",
      "epoch 96 : .......... average_train_NLL = 4.5234, oracle_sample_NLL = 10.9461\n",
      "epoch 97 : .......... average_train_NLL = 4.5213, oracle_sample_NLL = 10.9139\n",
      "epoch 98 : .......... average_train_NLL = 4.5194, oracle_sample_NLL = 10.9230\n",
      "epoch 99 : .......... average_train_NLL = 4.5298, oracle_sample_NLL = 10.9627\n",
      "epoch 100 : .......... average_train_NLL = 4.5203, oracle_sample_NLL = 10.9189\n"
     ]
    }
   ],
   "source": [
    "# GENERATOR MLE TRAINING\n",
    "print('Starting Generator MLE Training...')\n",
    "\n",
    "train_generator_MLE(gen, gen_optimizer, oracle, oracle_samples, \n",
    "                    MLE_TRAIN_EPOCHS, \n",
    "                    POS_NEG_SAMPLES = POS_NEG_SAMPLES,\n",
    "                    BATCH_SIZE = BATCH_SIZE,\n",
    "                    START_LETTER = START_LETTER,\n",
    "                    MAX_SEQ_LEN = MAX_SEQ_LEN,\n",
    "                    CUDA = CUDA,\n",
    ")\n",
    "# this will be a GPU version if you train on GPU, \n",
    "# if you train on cpu it will be a cpu model and there is\n",
    "# no need for the last line save\n",
    "torch.save(gen.state_dict(), pretrained_gen_path)\n",
    "torch.save(gen.cpu().state_dict(), pretrained_gen_path_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2125d055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Discriminator Training...\n",
      "d-step 1 epoch 1 : .......... average_loss = 0.6870, train_acc = 0.5441, val_acc = 0.5150\n",
      "d-step 1 epoch 2 : .......... average_loss = 0.6596, train_acc = 0.6076, val_acc = 0.5700\n",
      "d-step 1 epoch 3 : .......... average_loss = 0.6233, train_acc = 0.6573, val_acc = 0.6400\n",
      "d-step 2 epoch 1 : .......... average_loss = 0.6279, train_acc = 0.6466, val_acc = 0.6300\n",
      "d-step 2 epoch 2 : .......... average_loss = 0.5985, train_acc = 0.6801, val_acc = 0.6550\n",
      "d-step 2 epoch 3 : .......... average_loss = 0.5675, train_acc = 0.7090, val_acc = 0.6450\n",
      "d-step 3 epoch 1 : .......... average_loss = 0.5744, train_acc = 0.7015, val_acc = 0.6350\n",
      "d-step 3 epoch 2 : .......... average_loss = 0.5436, train_acc = 0.7280, val_acc = 0.6250\n",
      "d-step 3 epoch 3 : .......... average_loss = 0.5148, train_acc = 0.7490, val_acc = 0.6500\n",
      "d-step 4 epoch 1 : .......... average_loss = 0.5344, train_acc = 0.7332, val_acc = 0.6650\n",
      "d-step 4 epoch 2 : .......... average_loss = 0.5061, train_acc = 0.7534, val_acc = 0.6750\n",
      "d-step 4 epoch 3 : .......... average_loss = 0.4779, train_acc = 0.7752, val_acc = 0.6900\n",
      "d-step 5 epoch 1 : .......... average_loss = 0.4937, train_acc = 0.7634, val_acc = 0.6650\n",
      "d-step 5 epoch 2 : .......... average_loss = 0.4631, train_acc = 0.7862, val_acc = 0.6650\n",
      "d-step 5 epoch 3 : .......... average_loss = 0.4350, train_acc = 0.8011, val_acc = 0.6850\n",
      "d-step 6 epoch 1 : .......... average_loss = 0.4694, train_acc = 0.7792, val_acc = 0.6900\n",
      "d-step 6 epoch 2 : .......... average_loss = 0.4389, train_acc = 0.8030, val_acc = 0.6450\n",
      "d-step 6 epoch 3 : .......... average_loss = 0.4128, train_acc = 0.8175, val_acc = 0.6750\n",
      "d-step 7 epoch 1 : .......... average_loss = 0.4417, train_acc = 0.8004, val_acc = 0.7000\n",
      "d-step 7 epoch 2 : .......... average_loss = 0.4125, train_acc = 0.8196, val_acc = 0.6850\n",
      "d-step 7 epoch 3 : .......... average_loss = 0.3846, train_acc = 0.8333, val_acc = 0.6950\n",
      "d-step 8 epoch 1 : .......... average_loss = 0.4125, train_acc = 0.8181, val_acc = 0.6850\n",
      "d-step 8 epoch 2 : .......... average_loss = 0.3837, train_acc = 0.8336, val_acc = 0.6750\n",
      "d-step 8 epoch 3 : .......... average_loss = 0.3554, train_acc = 0.8489, val_acc = 0.7050\n",
      "d-step 9 epoch 1 : .......... average_loss = 0.3918, train_acc = 0.8322, val_acc = 0.6850\n",
      "d-step 9 epoch 2 : .......... average_loss = 0.3615, train_acc = 0.8491, val_acc = 0.7000\n",
      "d-step 9 epoch 3 : .......... average_loss = 0.3374, train_acc = 0.8633, val_acc = 0.7000\n",
      "d-step 10 epoch 1 : .......... average_loss = 0.3630, train_acc = 0.8514, val_acc = 0.7300\n",
      "d-step 10 epoch 2 : .......... average_loss = 0.3350, train_acc = 0.8661, val_acc = 0.7450\n",
      "d-step 10 epoch 3 : .......... average_loss = 0.3118, train_acc = 0.8756, val_acc = 0.7150\n",
      "d-step 11 epoch 1 : .......... average_loss = 0.3408, train_acc = 0.8638, val_acc = 0.7150\n",
      "d-step 11 epoch 2 : .......... average_loss = 0.3099, train_acc = 0.8790, val_acc = 0.6950\n",
      "d-step 11 epoch 3 : .......... average_loss = 0.2847, train_acc = 0.8910, val_acc = 0.7000\n",
      "d-step 12 epoch 1 : .......... average_loss = 0.3164, train_acc = 0.8742, val_acc = 0.7000\n",
      "d-step 12 epoch 2 : .......... average_loss = 0.2875, train_acc = 0.8907, val_acc = 0.7100\n",
      "d-step 12 epoch 3 : .......... average_loss = 0.2598, train_acc = 0.9020, val_acc = 0.6750\n",
      "d-step 13 epoch 1 : .......... average_loss = 0.3075, train_acc = 0.8821, val_acc = 0.6950\n",
      "d-step 13 epoch 2 : .......... average_loss = 0.2748, train_acc = 0.8979, val_acc = 0.7100\n",
      "d-step 13 epoch 3 : .......... average_loss = 0.2488, train_acc = 0.9075, val_acc = 0.7000\n",
      "d-step 14 epoch 1 : .......... average_loss = 0.2867, train_acc = 0.8932, val_acc = 0.7150\n",
      "d-step 14 epoch 2 : .......... average_loss = 0.2602, train_acc = 0.9062, val_acc = 0.6950\n",
      "d-step 14 epoch 3 : .......... average_loss = 0.2362, train_acc = 0.9175, val_acc = 0.7000\n",
      "d-step 15 epoch 1 : .......... average_loss = 0.2713, train_acc = 0.9027, val_acc = 0.7050\n",
      "d-step 15 epoch 2 : .......... average_loss = 0.2410, train_acc = 0.9170, val_acc = 0.7050\n",
      "d-step 15 epoch 3 : .......... average_loss = 0.2169, train_acc = 0.9248, val_acc = 0.7150\n",
      "d-step 16 epoch 1 : .......... average_loss = 0.2621, train_acc = 0.9062, val_acc = 0.6700\n",
      "d-step 16 epoch 2 : .......... average_loss = 0.2323, train_acc = 0.9197, val_acc = 0.6850\n",
      "d-step 16 epoch 3 : .......... average_loss = 0.2069, train_acc = 0.9291, val_acc = 0.7050\n",
      "d-step 17 epoch 1 : .......... average_loss = 0.2383, train_acc = 0.9202, val_acc = 0.6850\n",
      "d-step 17 epoch 2 : .......... average_loss = 0.2122, train_acc = 0.9321, val_acc = 0.7000\n",
      "d-step 17 epoch 3 : .......... average_loss = 0.1894, train_acc = 0.9393, val_acc = 0.6950\n",
      "d-step 18 epoch 1 : .......... average_loss = 0.2355, train_acc = 0.9217, val_acc = 0.6800\n",
      "d-step 18 epoch 2 : .......... average_loss = 0.2094, train_acc = 0.9317, val_acc = 0.6950\n",
      "d-step 18 epoch 3 : .......... average_loss = 0.1885, train_acc = 0.9407, val_acc = 0.6800\n",
      "d-step 19 epoch 1 : .......... average_loss = 0.2220, train_acc = 0.9284, val_acc = 0.6900\n",
      "d-step 19 epoch 2 : .......... average_loss = 0.1976, train_acc = 0.9388, val_acc = 0.6750\n",
      "d-step 19 epoch 3 : .......... average_loss = 0.1762, train_acc = 0.9443, val_acc = 0.6700\n",
      "d-step 20 epoch 1 : .......... average_loss = 0.2119, train_acc = 0.9327, val_acc = 0.6750\n",
      "d-step 20 epoch 2 : .......... average_loss = 0.1865, train_acc = 0.9416, val_acc = 0.6750\n",
      "d-step 20 epoch 3 : .......... average_loss = 0.1658, train_acc = 0.9484, val_acc = 0.6550\n",
      "d-step 21 epoch 1 : .......... average_loss = 0.2093, train_acc = 0.9345, val_acc = 0.6900\n",
      "d-step 21 epoch 2 : .......... average_loss = 0.1777, train_acc = 0.9454, val_acc = 0.6600\n",
      "d-step 21 epoch 3 : .......... average_loss = 0.1581, train_acc = 0.9515, val_acc = 0.6600\n",
      "d-step 22 epoch 1 : .......... average_loss = 0.1994, train_acc = 0.9387, val_acc = 0.6750\n",
      "d-step 22 epoch 2 : .......... average_loss = 0.1760, train_acc = 0.9465, val_acc = 0.6700\n",
      "d-step 22 epoch 3 : .......... average_loss = 0.1551, train_acc = 0.9547, val_acc = 0.6800\n",
      "d-step 23 epoch 1 : .......... average_loss = 0.1917, train_acc = 0.9409, val_acc = 0.6750\n",
      "d-step 23 epoch 2 : .......... average_loss = 0.1666, train_acc = 0.9493, val_acc = 0.6650\n",
      "d-step 23 epoch 3 : .......... average_loss = 0.1469, train_acc = 0.9554, val_acc = 0.6750\n",
      "d-step 24 epoch 1 : .......... average_loss = 0.1842, train_acc = 0.9459, val_acc = 0.6900\n",
      "d-step 24 epoch 2 : .......... average_loss = 0.1567, train_acc = 0.9549, val_acc = 0.6850\n",
      "d-step 24 epoch 3 : .......... average_loss = 0.1407, train_acc = 0.9615, val_acc = 0.6850\n",
      "d-step 25 epoch 1 : .......... average_loss = 0.1864, train_acc = 0.9449, val_acc = 0.6850\n",
      "d-step 25 epoch 2 : .......... average_loss = 0.1621, train_acc = 0.9518, val_acc = 0.6800\n",
      "d-step 25 epoch 3 : .......... average_loss = 0.1439, train_acc = 0.9594, val_acc = 0.6850\n",
      "d-step 26 epoch 1 : .......... average_loss = 0.1780, train_acc = 0.9489, val_acc = 0.6750\n",
      "d-step 26 epoch 2 : .......... average_loss = 0.1540, train_acc = 0.9548, val_acc = 0.6700\n",
      "d-step 26 epoch 3 : .......... average_loss = 0.1364, train_acc = 0.9607, val_acc = 0.6750\n",
      "d-step 27 epoch 1 : .......... average_loss = 0.1699, train_acc = 0.9533, val_acc = 0.6600\n",
      "d-step 27 epoch 2 : .......... average_loss = 0.1490, train_acc = 0.9583, val_acc = 0.6450\n",
      "d-step 27 epoch 3 : .......... average_loss = 0.1300, train_acc = 0.9634, val_acc = 0.6600\n",
      "d-step 28 epoch 1 : .......... average_loss = 0.1698, train_acc = 0.9520, val_acc = 0.6750\n",
      "d-step 28 epoch 2 : .......... average_loss = 0.1479, train_acc = 0.9586, val_acc = 0.6800\n",
      "d-step 28 epoch 3 : .......... average_loss = 0.1306, train_acc = 0.9642, val_acc = 0.6750\n",
      "d-step 29 epoch 1 : .......... average_loss = 0.1668, train_acc = 0.9529, val_acc = 0.6300\n",
      "d-step 29 epoch 2 : .......... average_loss = 0.1420, train_acc = 0.9597, val_acc = 0.6600\n",
      "d-step 29 epoch 3 : .......... average_loss = 0.1267, train_acc = 0.9650, val_acc = 0.6450\n",
      "d-step 30 epoch 1 : .......... average_loss = 0.1592, train_acc = 0.9563, val_acc = 0.6700\n",
      "d-step 30 epoch 2 : .......... average_loss = 0.1358, train_acc = 0.9609, val_acc = 0.6650\n",
      "d-step 30 epoch 3 : .......... average_loss = 0.1201, train_acc = 0.9660, val_acc = 0.6600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d-step 31 epoch 1 : .......... average_loss = 0.1492, train_acc = 0.9579, val_acc = 0.6550\n",
      "d-step 31 epoch 2 : .......... average_loss = 0.1272, train_acc = 0.9653, val_acc = 0.6750\n",
      "d-step 31 epoch 3 : .......... average_loss = 0.1086, train_acc = 0.9708, val_acc = 0.6700\n",
      "d-step 32 epoch 1 : .......... average_loss = 0.1490, train_acc = 0.9578, val_acc = 0.6450\n",
      "d-step 32 epoch 2 : .......... average_loss = 0.1249, train_acc = 0.9645, val_acc = 0.6600\n",
      "d-step 32 epoch 3 : .......... average_loss = 0.1087, train_acc = 0.9690, val_acc = 0.6700\n",
      "d-step 33 epoch 1 : .......... average_loss = 0.1517, train_acc = 0.9588, val_acc = 0.6650\n",
      "d-step 33 epoch 2 : .......... average_loss = 0.1267, train_acc = 0.9661, val_acc = 0.6650\n",
      "d-step 33 epoch 3 : .......... average_loss = 0.1112, train_acc = 0.9701, val_acc = 0.6700\n",
      "d-step 34 epoch 1 : .......... average_loss = 0.1403, train_acc = 0.9619, val_acc = 0.6550\n",
      "d-step 34 epoch 2 : .......... average_loss = 0.1201, train_acc = 0.9677, val_acc = 0.6750\n",
      "d-step 34 epoch 3 : .......... average_loss = 0.1038, train_acc = 0.9709, val_acc = 0.6700\n",
      "d-step 35 epoch 1 : .......... average_loss = 0.1423, train_acc = 0.9612, val_acc = 0.6700\n",
      "d-step 35 epoch 2 : .......... average_loss = 0.1187, train_acc = 0.9681, val_acc = 0.6650\n",
      "d-step 35 epoch 3 : .......... average_loss = 0.1050, train_acc = 0.9728, val_acc = 0.6650\n",
      "d-step 36 epoch 1 : .......... average_loss = 0.1302, train_acc = 0.9647, val_acc = 0.6600\n",
      "d-step 36 epoch 2 : .......... average_loss = 0.1095, train_acc = 0.9698, val_acc = 0.6550\n",
      "d-step 36 epoch 3 : .......... average_loss = 0.0933, train_acc = 0.9745, val_acc = 0.6700\n",
      "d-step 37 epoch 1 : .......... average_loss = 0.1339, train_acc = 0.9636, val_acc = 0.6550\n",
      "d-step 37 epoch 2 : .......... average_loss = 0.1137, train_acc = 0.9694, val_acc = 0.6700\n",
      "d-step 37 epoch 3 : .......... average_loss = 0.0997, train_acc = 0.9724, val_acc = 0.6550\n",
      "d-step 38 epoch 1 : .......... average_loss = 0.1336, train_acc = 0.9636, val_acc = 0.6350\n",
      "d-step 38 epoch 2 : .......... average_loss = 0.1098, train_acc = 0.9718, val_acc = 0.6350\n",
      "d-step 38 epoch 3 : .......... average_loss = 0.0960, train_acc = 0.9741, val_acc = 0.6550\n",
      "d-step 39 epoch 1 : .......... average_loss = 0.1254, train_acc = 0.9663, val_acc = 0.6450\n",
      "d-step 39 epoch 2 : .......... average_loss = 0.1037, train_acc = 0.9718, val_acc = 0.6450\n",
      "d-step 39 epoch 3 : .......... average_loss = 0.0885, train_acc = 0.9756, val_acc = 0.6500\n",
      "d-step 40 epoch 1 : .......... average_loss = 0.1275, train_acc = 0.9664, val_acc = 0.6600\n",
      "d-step 40 epoch 2 : .......... average_loss = 0.1085, train_acc = 0.9719, val_acc = 0.6450\n",
      "d-step 40 epoch 3 : .......... average_loss = 0.0934, train_acc = 0.9763, val_acc = 0.6600\n",
      "d-step 41 epoch 1 : .......... average_loss = 0.1176, train_acc = 0.9686, val_acc = 0.6500\n",
      "d-step 41 epoch 2 : .......... average_loss = 0.0985, train_acc = 0.9738, val_acc = 0.6450\n",
      "d-step 41 epoch 3 : .......... average_loss = 0.0820, train_acc = 0.9793, val_acc = 0.6400\n",
      "d-step 42 epoch 1 : .......... average_loss = 0.1308, train_acc = 0.9669, val_acc = 0.6550\n",
      "d-step 42 epoch 2 : .......... average_loss = 0.1086, train_acc = 0.9710, val_acc = 0.6450\n",
      "d-step 42 epoch 3 : .......... average_loss = 0.0939, train_acc = 0.9757, val_acc = 0.6500\n",
      "d-step 43 epoch 1 : .......... average_loss = 0.1186, train_acc = 0.9678, val_acc = 0.6500\n",
      "d-step 43 epoch 2 : .......... average_loss = 0.0988, train_acc = 0.9738, val_acc = 0.6550\n",
      "d-step 43 epoch 3 : .......... average_loss = 0.0833, train_acc = 0.9779, val_acc = 0.6550\n",
      "d-step 44 epoch 1 : .......... average_loss = 0.1189, train_acc = 0.9680, val_acc = 0.6450\n",
      "d-step 44 epoch 2 : .......... average_loss = 0.1006, train_acc = 0.9740, val_acc = 0.6400\n",
      "d-step 44 epoch 3 : .......... average_loss = 0.0863, train_acc = 0.9782, val_acc = 0.6300\n",
      "d-step 45 epoch 1 : .......... average_loss = 0.1143, train_acc = 0.9702, val_acc = 0.6450\n",
      "d-step 45 epoch 2 : .......... average_loss = 0.0952, train_acc = 0.9745, val_acc = 0.6400\n",
      "d-step 45 epoch 3 : .......... average_loss = 0.0806, train_acc = 0.9798, val_acc = 0.6500\n",
      "d-step 46 epoch 1 : .......... average_loss = 0.1166, train_acc = 0.9699, val_acc = 0.6400\n",
      "d-step 46 epoch 2 : .......... average_loss = 0.0982, train_acc = 0.9739, val_acc = 0.6450\n",
      "d-step 46 epoch 3 : .......... average_loss = 0.0841, train_acc = 0.9784, val_acc = 0.6500\n",
      "d-step 47 epoch 1 : .......... average_loss = 0.1212, train_acc = 0.9703, val_acc = 0.6250\n",
      "d-step 47 epoch 2 : .......... average_loss = 0.0994, train_acc = 0.9756, val_acc = 0.6500\n",
      "d-step 47 epoch 3 : .......... average_loss = 0.0879, train_acc = 0.9774, val_acc = 0.6450\n",
      "d-step 48 epoch 1 : .......... average_loss = 0.0998, train_acc = 0.9757, val_acc = 0.6450\n",
      "d-step 48 epoch 2 : .......... average_loss = 0.0858, train_acc = 0.9772, val_acc = 0.6350\n",
      "d-step 48 epoch 3 : .......... average_loss = 0.0743, train_acc = 0.9814, val_acc = 0.6550\n",
      "d-step 49 epoch 1 : .......... average_loss = 0.1099, train_acc = 0.9722, val_acc = 0.6400\n",
      "d-step 49 epoch 2 : .......... average_loss = 0.0885, train_acc = 0.9774, val_acc = 0.6400\n",
      "d-step 49 epoch 3 : .......... average_loss = 0.0744, train_acc = 0.9810, val_acc = 0.6350\n",
      "d-step 50 epoch 1 : .......... average_loss = 0.1018, train_acc = 0.9728, val_acc = 0.6400\n",
      "d-step 50 epoch 2 : .......... average_loss = 0.0874, train_acc = 0.9778, val_acc = 0.6400\n",
      "d-step 50 epoch 3 : .......... average_loss = 0.0711, train_acc = 0.9818, val_acc = 0.6400\n"
     ]
    }
   ],
   "source": [
    "# PRETRAIN DISCRIMINATOR\n",
    "print('Starting Discriminator Training...')\n",
    "\n",
    "train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, \n",
    "                    d_steps = 50,  \n",
    "                    epochs = 3,\n",
    "                    POS_NEG_SAMPLES = POS_NEG_SAMPLES,\n",
    "                    BATCH_SIZE = BATCH_SIZE,\n",
    "                    CUDA = CUDA,\n",
    ")\n",
    "# this will be a GPU version if you train on GPU, \n",
    "# if you train on cpu it will be a cpu model and there is\n",
    "# no need for the last line save\n",
    "torch.save(dis.state_dict(), pretrained_dis_path)\n",
    "torch.save(dis.cpu().state_dict(), pretrained_dis_path_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570be696",
   "metadata": {},
   "source": [
    "### Sanity Check 1\n",
    "\n",
    "At the end of pretraining the generator shuld have anegative log likelihood loss of around 10\n",
    "```\n",
    "tarting Generator MLE Training...\n",
    "epoch 100 : .......... average_train_NLL = 4.5203, oracle_sample_NLL = 10.9189\n",
    "```\n",
    "\n",
    "meaning it is good enough tha the discriminator only is correct about half the time initially\n",
    "\n",
    "```\n",
    "Starting Discriminator Training...\n",
    "d-step 1 epoch 1 : .......... average_loss = 0.6870, train_acc = 0.5441, val_acc = 0.5150\n",
    ".\n",
    ".\n",
    ".\n",
    "d-step 30 epoch 2 : .......... average_loss = 0.1358, train_acc = 0.9609, val_acc = 0.6650\n",
    "```\n",
    "\n",
    "the discriminator will make much more gains on the training accuracy than on the validation accuracy\n",
    "\n",
    "### Load Pretrained Generator and Discriminator\n",
    "\n",
    "You can load the CPU version and sent to GPU or load the GPU version for a model already on CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "880002f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained generator and discrimnator for GPU Models\n",
    "# en.load_state_dict(torch.load(pretrained_gen_path))\n",
    "# dis.load_state_dict(torch.load(pretrained_dis_path))\n",
    "\n",
    "# load pretrained generator and discrimnator for CPU Models\n",
    "gen.load_state_dict(torch.load(pretrained_gen_path_cpu))\n",
    "dis.load_state_dict(torch.load(pretrained_dis_path_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d037c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Oracle Sample Loss : 10.9625\n",
      "CPU times: user 22 s, sys: 41 ms, total: 22.1 s\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sample from generator and compute oracle NLL\n",
    "oracle_loss = helpers.batchwise_oracle_nll(\n",
    "    gen, \n",
    "    oracle, \n",
    "    POS_NEG_SAMPLES, \n",
    "    BATCH_SIZE, MAX_SEQ_LEN,\n",
    "    start_letter=START_LETTER, \n",
    "    gpu=CUDA,\n",
    ")\n",
    "\n",
    "print('\\nInitial Oracle Sample Loss : %.4f' % oracle_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b637ed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Adversarial Training...\n",
      "\n",
      "--------\n",
      "EPOCH 1\n",
      "--------\n",
      "\n",
      "Adversarial Training Generator :  oracle_sample_NLL = 10.9642\n",
      "\n",
      "Adversarial Training Discriminator : \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# TRAIN DISCRIMINATOR\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAdversarial Training Discriminator : \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, \n\u001b[1;32m     18\u001b[0m                     d_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,  \n\u001b[1;32m     19\u001b[0m                     epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     20\u001b[0m                     POS_NEG_SAMPLES \u001b[38;5;241m=\u001b[39m POS_NEG_SAMPLES,\n\u001b[1;32m     21\u001b[0m                     BATCH_SIZE \u001b[38;5;241m=\u001b[39m BATCH_SIZE,\n\u001b[1;32m     22\u001b[0m                     CUDA \u001b[38;5;241m=\u001b[39m CUDA,\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/projects/seqGAN/notebooks/../trainers.py:111\u001b[0m, in \u001b[0;36mtrain_discriminator\u001b[0;34m(discriminator, dis_opt, real_data_samples, generator, oracle, d_steps, epochs, POS_NEG_SAMPLES, BATCH_SIZE, CUDA)\u001b[0m\n\u001b[1;32m    108\u001b[0m val_inp, val_target \u001b[38;5;241m=\u001b[39m helpers\u001b[38;5;241m.\u001b[39mprepare_discriminator_data(pos_val, neg_val, gpu\u001b[38;5;241m=\u001b[39mCUDA)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_steps):\n\u001b[0;32m--> 111\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mhelpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatchwise_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPOS_NEG_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     dis_inp, dis_target \u001b[38;5;241m=\u001b[39m helpers\u001b[38;5;241m.\u001b[39mprepare_discriminator_data(real_data_samples, s, gpu\u001b[38;5;241m=\u001b[39mCUDA)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "File \u001b[0;32m~/projects/seqGAN/notebooks/../helpers.py:74\u001b[0m, in \u001b[0;36mbatchwise_sample\u001b[0;34m(gen, num_samples, batch_size)\u001b[0m\n\u001b[1;32m     72\u001b[0m samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(ceil(num_samples\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(batch_size)))):\n\u001b[0;32m---> 74\u001b[0m     samples\u001b[38;5;241m.\u001b[39mappend(\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(samples, \u001b[38;5;241m0\u001b[39m)[:num_samples]\n",
      "File \u001b[0;32m~/projects/seqGAN/notebooks/../models/generator.py:72\u001b[0m, in \u001b[0;36mGenerator.sample\u001b[0;34m(self, num_samples, start_letter)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len):\n\u001b[1;32m     71\u001b[0m     out, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(inp, h)               \u001b[38;5;66;03m# out: num_samples x vocab_size\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# num_samples x 1 (sampling from each row)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     samples[:, i] \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     75\u001b[0m     inp \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ADVERSARIAL TRAINING\n",
    "print('\\nStarting Adversarial Training...')\n",
    "\n",
    "for epoch in range(ADV_TRAIN_EPOCHS):\n",
    "    \n",
    "    print('\\n--------\\nEPOCH %d\\n--------' % (epoch+1))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # TRAIN GENERATOR\n",
    "    print('\\nAdversarial Training Generator : ', end='')\n",
    "    train_generator_PG(gen, gen_optimizer, oracle, dis, \n",
    "                       num_batches = 1,\n",
    "    )\n",
    "\n",
    "    # TRAIN DISCRIMINATOR\n",
    "    print('\\nAdversarial Training Discriminator : ')\n",
    "    train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, \n",
    "                        d_steps = 50,  \n",
    "                        epochs = 3,\n",
    "                        POS_NEG_SAMPLES = POS_NEG_SAMPLES,\n",
    "                        BATCH_SIZE = BATCH_SIZE,\n",
    "                        CUDA = CUDA,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90014e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
