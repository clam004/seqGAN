{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e41a2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from models import generator\n",
    "from models import discriminator\n",
    "from trainers import train_generator_MLE, train_generator_PG, train_discriminator\n",
    "import helpers\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885703f8",
   "metadata": {},
   "source": [
    "# Tutorial Part 1 Overall Story\n",
    "\n",
    "here we will describe each step of the pipeline at a high level to help us contextualize our future learnings\n",
    "\n",
    "\n",
    "## Synthetic Data Experiment\n",
    "\n",
    "The most accurate way of evaluating generative models is that we draw some samples from it and let human observers re- view them based on their prior knowledge. We assume that the human observer has learned an accurate model of the natural distribution p_human(x). \n",
    " \n",
    "the authots used a randomly initialized language model as the true model, aka, the ***oracle***, to generate the \"real\" data distribution p(x_t |x_1 , . . . , x_tâˆ’1 ). The benefit of having such oracle is that firstly, it provides the training dataset and secondly evaluates the exact perfor- mance of the generative models, which will not be possible with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d109bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental constants\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_SEQ_LEN = 20\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "START_LETTER = 0\n",
    "\n",
    "GEN_EMBEDDING_DIM = 32 # length of input vectors for generator and oracle\n",
    "GEN_HIDDEN_DIM = 32 # length of hidden state for generator and oracle\n",
    "\n",
    "oracle_state_dict_path = '../params/oracle_EMBDIM32_HIDDENDIM32_VOCAB5000_MAXSEQLEN20.trc'\n",
    "oracle_samples_path = '../sample_data/oracle_samples.trc'\n",
    "\n",
    "MLE_TRAIN_EPOCHS = 100\n",
    "POS_NEG_SAMPLES = 10000\n",
    "\n",
    "DIS_EMBEDDING_DIM = 64 # length of input vectors for discriminator\n",
    "DIS_HIDDEN_DIM = 64 # length of hidden state for discriminator\n",
    "\n",
    "pretrained_gen_path = '../params/gen_MLEtrain_EMBDIM32_HIDDENDIM32_VOCAB5000_MAXSEQLEN20.trc'\n",
    "pretrained_dis_path = '../params/dis_pretrain_EMBDIM_64_HIDDENDIM64_VOCAB5000_MAXSEQLEN20.trc'\n",
    "\n",
    "pretrained_gen_path_cpu = '../params/gen_MLEtrain_EMBDIM32_HIDDENDIM32_VOCAB5000_MAXSEQLEN20_cpu.trc'\n",
    "pretrained_dis_path_cpu = '../params/dis_pretrain_EMBDIM_64_HIDDENDIM64_VOCAB5000_MAXSEQLEN20_cpu.trc'\n",
    "\n",
    "ADV_TRAIN_EPOCHS = 50 # ADVERSARIAL TRAINING EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572ec55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (embeddings): Embedding(5000, 32)\n",
       "  (gru): GRU(32, 32)\n",
       "  (gru2out): Linear(in_features=32, out_features=5000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle = generator.Generator(\n",
    "    GEN_EMBEDDING_DIM, \n",
    "    GEN_HIDDEN_DIM, \n",
    "    VOCAB_SIZE, \n",
    "    MAX_SEQ_LEN, \n",
    "    gpu=CUDA\n",
    ")\n",
    "\n",
    "# for reproducibiility we provide saved parameters for the oracle\n",
    "oracle.load_state_dict(torch.load(oracle_state_dict_path))\n",
    "\n",
    "oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7bd54",
   "metadata": {},
   "source": [
    "The output above should look like this\n",
    "\n",
    "```\n",
    "Generator(\n",
    "  (embeddings): Embedding(5000, 32)\n",
    "  (gru): GRU(32, 32)\n",
    "  (gru2out): Linear(in_features=32, out_features=5000, bias=True)\n",
    ")\n",
    "```\n",
    "\n",
    "To explain the information above: the model has 5000 possible input vectors in its vocab each with length 32, the GRU takes vectors  length 32 and outputs activation of the same length. The output returns activations of the same length as the vocab.\n",
    "\n",
    "the authors use the oracle to generate 10,000 sequences of length 20 as the training set S for the generative models.\n",
    "we have already used helpers.batchwise_sample() to save S so you can load it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4715013d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([10000, 20])\n"
     ]
    }
   ],
   "source": [
    "oracle_samples = torch.load(oracle_samples_path).type(torch.LongTensor)\n",
    "print(type(oracle_samples), oracle_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97b2571",
   "metadata": {},
   "source": [
    "### instantiate a generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d93c36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generator.Generator(\n",
    "    GEN_EMBEDDING_DIM, \n",
    "    GEN_HIDDEN_DIM, \n",
    "    VOCAB_SIZE, \n",
    "    MAX_SEQ_LEN, \n",
    "    gpu=CUDA,\n",
    ")\n",
    "\n",
    "dis = discriminator.Discriminator(\n",
    "    DIS_EMBEDDING_DIM, \n",
    "    DIS_HIDDEN_DIM, \n",
    "    VOCAB_SIZE,\n",
    "    MAX_SEQ_LEN, \n",
    "    gpu=CUDA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c488af86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (embeddings): Embedding(5000, 32)\n",
       "  (gru): GRU(32, 32)\n",
       "  (gru2out): Linear(in_features=32, out_features=5000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d8f237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (embeddings): Embedding(5000, 64)\n",
       "  (gru): GRU(64, 64, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "  (gru2hidden): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (dropout_linear): Dropout(p=0.2, inplace=False)\n",
       "  (hidden2out): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc76ff",
   "metadata": {},
   "source": [
    "#### If you have and want to use GPU, all models and model inputs need to be on GPU\n",
    "\n",
    "If there is any mismatch between the parameters being on one device and the inputs being on another, then problems will arise. \n",
    "\n",
    "We run one call to the helpers.batchwise_oracle_nll function to test that we have the inputs and params on mathing devices, to get the baseline oracle NLL and to get a sense of the GPU/CPU speedup. On CPU the original batchwise_oracle_nll has a wall time of 22 seconds, which is lowered to 4 seconds on GPU.\n",
    "\n",
    "If you switch the models device, before doing a gradient update, you have to re-assign the parameters at theri new location to the optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7de81e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "if CUDA:\n",
    "    oracle = oracle.cuda()\n",
    "    gen = gen.cuda()\n",
    "    dis = dis.cuda()\n",
    "    oracle_samples = oracle_samples.cuda()\n",
    "    \n",
    "    print(torch.cuda.device_count(), torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dbaabc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 cuda:0 cuda:0 True\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    next(gen.embeddings.parameters()).device, \n",
    "    next(dis.embeddings.parameters()).device,\n",
    "    next(oracle.embeddings.parameters()).device,\n",
    "    oracle_samples.is_cuda\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ebb05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = optim.Adam(gen.parameters(), lr=1e-2)\n",
    "dis_optimizer = optim.Adagrad(dis.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb298fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracle_loss 18.073519830322265\n",
      "CPU times: user 4.34 s, sys: 45.7 ms, total: 4.38 s\n",
      "Wall time: 4.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sample from generator and compute baseline oracle NLL\n",
    "oracle_loss = helpers.batchwise_oracle_nll(\n",
    "    gen, \n",
    "    oracle, \n",
    "    POS_NEG_SAMPLES, \n",
    "    BATCH_SIZE, MAX_SEQ_LEN,\n",
    "    start_letter=START_LETTER, \n",
    "    gpu=CUDA,\n",
    ")\n",
    "\n",
    "print('oracle_loss', oracle_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN DISCRIMINATOR\n",
    "print('\\nAdversarial Training Discriminator : ')\n",
    "train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, \n",
    "                    d_steps = 50,  \n",
    "                    epochs = 3,\n",
    "                    POS_NEG_SAMPLES = POS_NEG_SAMPLES,\n",
    "                    BATCH_SIZE = BATCH_SIZE,\n",
    "                    CUDA = CUDA,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf5a57",
   "metadata": {},
   "source": [
    "### GENERATOR MLE TRAINING\n",
    "\n",
    "At the beginning of the training, the authors used maximum likelihood estimation (MLE) to pretrain GÎ¸ on training set S. \n",
    "\n",
    "They found the supervised signal from the pretrained discriminator is informative to help adjust the generator efficiently.\n",
    "\n",
    "```\n",
    "# GENERATOR MLE TRAINING\n",
    "print('Starting Generator MLE Training...')\n",
    "gen_optimizer = optim.Adam(gen.parameters(), lr=1e-2)\n",
    "train_generator_MLE(gen, gen_optimizer, oracle, oracle_samples, MLE_TRAIN_EPOCHS)\n",
    "torch.save(gen.state_dict(), pretrained_gen_path)\n",
    "\n",
    "# PRETRAIN DISCRIMINATOR\n",
    "print('Starting Discriminator Training...')\n",
    "dis_optimizer = optim.Adagrad(dis.parameters())\n",
    "train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, d_steps = 50,  epochs = 3)\n",
    "torch.save(dis.state_dict(), pretrained_dis_path)\n",
    "```\n",
    "\n",
    "The below pretraining only needs to be done once. After it is saved, it can be loaded using `model.load_state_dict(torch.load(pretrained_gen_path))` in the cell two cells down while skipping the next two cells. You can skip to ***Load Pretrained Generator and Discriminator***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2de2c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Generator MLE Training...\n",
      "epoch 1 : .......... average_train_NLL = 6.8282, oracle_sample_NLL = 14.6160\n",
      "epoch 2 : .......... average_train_NLL = 6.1780, oracle_sample_NLL = 13.7436\n",
      "epoch 3 : .......... average_train_NLL = 5.8610, oracle_sample_NLL = 13.1479\n",
      "epoch 4 : .......... average_train_NLL = 5.6560, oracle_sample_NLL = 12.8644\n",
      "epoch 5 : .......... average_train_NLL = 5.5087, oracle_sample_NLL = 12.5425\n",
      "epoch 6 : .......... average_train_NLL = 5.3974, oracle_sample_NLL = 12.3266\n",
      "epoch 7 : .......... average_train_NLL = 5.3087, oracle_sample_NLL = 12.1899\n",
      "epoch 8 : .......... average_train_NLL = 5.2363, oracle_sample_NLL = 12.0750\n",
      "epoch 9 : .......... average_train_NLL = 5.1747, oracle_sample_NLL = 11.9395\n",
      "epoch 10 : .......... average_train_NLL = 5.1223, oracle_sample_NLL = 11.8729\n",
      "epoch 11 : .......... average_train_NLL = 5.0776, oracle_sample_NLL = 11.8209\n",
      "epoch 12 : .......... average_train_NLL = 5.0383, oracle_sample_NLL = 11.7077\n",
      "epoch 13 : .......... average_train_NLL = 5.0031, oracle_sample_NLL = 11.6678\n",
      "epoch 14 : .......... average_train_NLL = 4.9722, oracle_sample_NLL = 11.5674\n",
      "epoch 15 : .......... average_train_NLL = 4.9448, oracle_sample_NLL = 11.5367\n",
      "epoch 16 : .......... average_train_NLL = 4.9199, oracle_sample_NLL = 11.4842\n",
      "epoch 17 : .......... average_train_NLL = 4.8971, oracle_sample_NLL = 11.5041\n",
      "epoch 18 : .......... average_train_NLL = 4.8772, oracle_sample_NLL = 11.4346\n",
      "epoch 19 : .......... average_train_NLL = 4.8570, oracle_sample_NLL = 11.4180\n",
      "epoch 20 : .......... average_train_NLL = 4.8420, oracle_sample_NLL = 11.3818\n",
      "epoch 21 : .......... average_train_NLL = 4.8253, oracle_sample_NLL = 11.3583\n",
      "epoch 22 : .......... average_train_NLL = 4.8114, oracle_sample_NLL = 11.3381\n",
      "epoch 23 : .......... average_train_NLL = 4.7979, oracle_sample_NLL = 11.2928\n",
      "epoch 24 : .......... average_train_NLL = 4.7840, oracle_sample_NLL = 11.2967\n",
      "epoch 25 : .......... average_train_NLL = 4.7722, oracle_sample_NLL = 11.2697\n",
      "epoch 26 : .......... average_train_NLL = 4.7633, oracle_sample_NLL = 11.2715\n",
      "epoch 27 : .......... average_train_NLL = 4.7525, oracle_sample_NLL = 11.2292\n",
      "epoch 28 : .......... average_train_NLL = 4.7441, oracle_sample_NLL = 11.2136\n",
      "epoch 29 : .......... average_train_NLL = 4.7346, oracle_sample_NLL = 11.2128\n",
      "epoch 30 : .......... average_train_NLL = 4.7258, oracle_sample_NLL = 11.1949\n",
      "epoch 31 : .......... average_train_NLL = 4.7178, oracle_sample_NLL = 11.1940\n",
      "epoch 32 : .......... average_train_NLL = 4.7085, oracle_sample_NLL = 11.1875\n",
      "epoch 33 : .......... average_train_NLL = 4.7041, oracle_sample_NLL = 11.1756\n",
      "epoch 34 : .......... average_train_NLL = 4.6953, oracle_sample_NLL = 11.1604\n",
      "epoch 35 : .......... average_train_NLL = 4.6908, oracle_sample_NLL = 11.1249\n",
      "epoch 36 : .......... average_train_NLL = 4.6830, oracle_sample_NLL = 11.1920\n",
      "epoch 37 : .......... average_train_NLL = 4.6793, oracle_sample_NLL = 11.1615\n",
      "epoch 38 : .......... average_train_NLL = 4.6713, oracle_sample_NLL = 11.1487\n",
      "epoch 39 : .......... average_train_NLL = 4.6641, oracle_sample_NLL = 11.0967\n",
      "epoch 40 : .......... average_train_NLL = 4.6587, oracle_sample_NLL = 11.1435\n",
      "epoch 41 : .......... average_train_NLL = 4.6581, oracle_sample_NLL = 11.1504\n",
      "epoch 42 : .......... average_train_NLL = 4.6557, oracle_sample_NLL = 11.0924\n",
      "epoch 43 : .......... average_train_NLL = 4.6442, oracle_sample_NLL = 11.0939\n",
      "epoch 44 : .......... average_train_NLL = 4.6413, oracle_sample_NLL = 11.0904\n",
      "epoch 45 : .......... average_train_NLL = 4.6350, oracle_sample_NLL = 11.1227\n",
      "epoch 46 : .......... average_train_NLL = 4.6360, oracle_sample_NLL = 11.0930\n",
      "epoch 47 : .......... average_train_NLL = 4.6324, oracle_sample_NLL = 11.0728\n",
      "epoch 48 : .......... average_train_NLL = 4.6271, oracle_sample_NLL = 11.0899\n",
      "epoch 49 : .......... average_train_NLL = 4.6214, oracle_sample_NLL = 11.0461\n",
      "epoch 50 : .......... average_train_NLL = 4.6182, oracle_sample_NLL = 11.0488\n",
      "epoch 51 : .......... average_train_NLL = 4.6208, oracle_sample_NLL = 11.1194\n",
      "epoch 52 : .......... average_train_NLL = 4.6134, oracle_sample_NLL = 11.0896\n",
      "epoch 53 : .......... average_train_NLL = 4.6083, oracle_sample_NLL = 11.0824\n",
      "epoch 54 : .......... average_train_NLL = 4.6047, oracle_sample_NLL = 11.0207\n",
      "epoch 55 : .......... average_train_NLL = 4.6029, oracle_sample_NLL = 11.0760\n",
      "epoch 56 : .......... average_train_NLL = 4.5962, oracle_sample_NLL = 11.0465\n",
      "epoch 57 : .......... average_train_NLL = 4.5984, oracle_sample_NLL = 11.0643\n",
      "epoch 58 : .......... average_train_NLL = 4.5932, oracle_sample_NLL = 11.0679\n",
      "epoch 59 : .......... average_train_NLL = 4.5862, oracle_sample_NLL = 11.0694\n",
      "epoch 60 : .......... average_train_NLL = 4.5828, oracle_sample_NLL = 11.0863\n",
      "epoch 61 : .......... average_train_NLL = 4.5823, oracle_sample_NLL = 11.0437\n",
      "epoch 62 : .......... average_train_NLL = 4.5846, oracle_sample_NLL = 11.0391\n",
      "epoch 63 : .......... average_train_NLL = 4.5784, oracle_sample_NLL = 11.0437\n",
      "epoch 64 : .......... average_train_NLL = 4.5776, oracle_sample_NLL = 11.0643\n",
      "epoch 65 : .......... average_train_NLL = 4.5773, oracle_sample_NLL = 11.0481\n",
      "epoch 66 : .......... average_train_NLL = 4.5728, oracle_sample_NLL = 11.0131\n",
      "epoch 67 : .......... average_train_NLL = 4.5712, oracle_sample_NLL = 11.0715\n",
      "epoch 68 : .......... average_train_NLL = 4.5695, oracle_sample_NLL = 11.0328\n",
      "epoch 69 : .......... average_train_NLL = 4.5669, oracle_sample_NLL = 11.0216\n",
      "epoch 70 : .......... average_train_NLL = 4.5602, oracle_sample_NLL = 11.0316\n",
      "epoch 71 : .......... average_train_NLL = 4.5619, oracle_sample_NLL = 10.9854\n",
      "epoch 72 : .......... average_train_NLL = 4.5612, oracle_sample_NLL = 11.0086\n",
      "epoch 73 : .......... average_train_NLL = 4.5794, oracle_sample_NLL = 11.0488\n",
      "epoch 74 : .......... average_train_NLL = 4.5559, oracle_sample_NLL = 10.9845\n",
      "epoch 75 : .......... average_train_NLL = 4.5568, oracle_sample_NLL = 10.9918\n",
      "epoch 76 : .......... average_train_NLL = 4.5527, oracle_sample_NLL = 10.9917\n",
      "epoch 77 : .......... average_train_NLL = 4.5528, oracle_sample_NLL = 10.9918\n",
      "epoch 78 : .......... average_train_NLL = 4.5539, oracle_sample_NLL = 10.9771\n",
      "epoch 79 : .......... average_train_NLL = 4.5429, oracle_sample_NLL = 10.9805\n",
      "epoch 80 : .......... average_train_NLL = 4.5412, oracle_sample_NLL = 10.9624\n",
      "epoch 81 : .......... average_train_NLL = 4.5406, oracle_sample_NLL = 10.9655\n",
      "epoch 82 : .......... average_train_NLL = 4.5381, oracle_sample_NLL = 10.9985\n",
      "epoch 83 : .......... average_train_NLL = 4.5443, oracle_sample_NLL = 10.9729\n",
      "epoch 84 : .......... average_train_NLL = 4.5395, oracle_sample_NLL = 10.9439\n",
      "epoch 85 : .......... average_train_NLL = 4.5384, oracle_sample_NLL = 10.9812\n",
      "epoch 86 : .......... average_train_NLL = 4.5381, oracle_sample_NLL = 10.9694\n",
      "epoch 87 : .......... average_train_NLL = 4.5347, oracle_sample_NLL = 10.9605\n",
      "epoch 88 : .......... average_train_NLL = 4.5463, oracle_sample_NLL = 10.9484\n",
      "epoch 89 : .......... average_train_NLL = 4.5331, oracle_sample_NLL = 10.9364\n",
      "epoch 90 : .......... average_train_NLL = 4.5292, oracle_sample_NLL = 10.9737\n",
      "epoch 91 : .......... average_train_NLL = 4.5300, oracle_sample_NLL = 10.9442\n",
      "epoch 92 : .......... average_train_NLL = 4.5254, oracle_sample_NLL = 10.9750\n",
      "epoch 93 : .......... average_train_NLL = 4.5268, oracle_sample_NLL = 10.9237\n",
      "epoch 94 : .......... average_train_NLL = 4.5202, oracle_sample_NLL = 10.9857\n",
      "epoch 95 : .......... average_train_NLL = 4.5195, oracle_sample_NLL = 10.9686\n",
      "epoch 96 : .......... average_train_NLL = 4.5234, oracle_sample_NLL = 10.9461\n",
      "epoch 97 : .......... average_train_NLL = 4.5213, oracle_sample_NLL = 10.9139\n",
      "epoch 98 : .......... average_train_NLL = 4.5194, oracle_sample_NLL = 10.9230\n",
      "epoch 99 : .......... average_train_NLL = 4.5298, oracle_sample_NLL = 10.9627\n",
      "epoch 100 : .......... average_train_NLL = 4.5203, oracle_sample_NLL = 10.9189\n"
     ]
    }
   ],
   "source": [
    "# GENERATOR MLE TRAINING\n",
    "print('Starting Generator MLE Training...')\n",
    "\n",
    "train_generator_MLE(gen, gen_optimizer, oracle, oracle_samples, \n",
    "                    MLE_TRAIN_EPOCHS, \n",
    "                    POS_NEG_SAMPLES = POS_NEG_SAMPLES,\n",
    "                    BATCH_SIZE = BATCH_SIZE,\n",
    "                    START_LETTER = START_LETTER,\n",
    "                    MAX_SEQ_LEN = MAX_SEQ_LEN,\n",
    "                    CUDA = CUDA,\n",
    ")\n",
    "# this will be a GPU version if you train on GPU, \n",
    "# if you train on cpu it will be a cpu model and there is\n",
    "# no need for the last line save\n",
    "torch.save(gen.state_dict(), pretrained_gen_path)\n",
    "torch.save(gen.cpu().state_dict(), pretrained_gen_path_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRETRAIN DISCRIMINATOR\n",
    "print('Starting Discriminator Training...')\n",
    "\n",
    "train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, \n",
    "                    d_steps = 50,  \n",
    "                    epochs = 3,\n",
    "                    POS_NEG_SAMPLES = POS_NEG_SAMPLES,\n",
    "                    BATCH_SIZE = BATCH_SIZE,\n",
    "                    CUDA = CUDA,\n",
    ")\n",
    "# this will be a GPU version if you train on GPU, \n",
    "# if you train on cpu it will be a cpu model and there is\n",
    "# no need for the last line save\n",
    "torch.save(dis.state_dict(), pretrained_dis_path)\n",
    "torch.save(dis.cpu().state_dict(), pretrained_dis_path_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570be696",
   "metadata": {},
   "source": [
    "### Sanity Check 1\n",
    "\n",
    "At the end of pretraining the generator shuld have anegative log likelihood loss of around 10\n",
    "```\n",
    "tarting Generator MLE Training...\n",
    "epoch 100 : .......... average_train_NLL = 4.5203, oracle_sample_NLL = 10.9189\n",
    "```\n",
    "\n",
    "meaning it is good enough tha the discriminator only is correct about half the time initially\n",
    "\n",
    "```\n",
    "Starting Discriminator Training...\n",
    "d-step 1 epoch 1 : .......... average_loss = 0.6870, train_acc = 0.5441, val_acc = 0.5150\n",
    ".\n",
    ".\n",
    ".\n",
    "d-step 30 epoch 2 : .......... average_loss = 0.1358, train_acc = 0.9609, val_acc = 0.6650\n",
    "```\n",
    "\n",
    "the discriminator will make much more gains on the training accuracy than on the validation accuracy\n",
    "\n",
    "### Load Pretrained Generator and Discriminator\n",
    "\n",
    "You can load the CPU version and sent to GPU or load the GPU version for a model already on CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "880002f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained generator and discrimnator for GPU Models\n",
    "gen.load_state_dict(torch.load(pretrained_gen_path))\n",
    "dis.load_state_dict(torch.load(pretrained_dis_path))\n",
    "\n",
    "# load pretrained generator and discrimnator for CPU Models\n",
    "# gen.load_state_dict(torch.load(pretrained_gen_path_cpu))\n",
    "# dis.load_state_dict(torch.load(pretrained_dis_path_cpu))\n",
    "\n",
    "# point optimizer to parameters on the correct device\n",
    "gen_optimizer = optim.Adam(gen.parameters(), lr=1e-2)\n",
    "dis_optimizer = optim.Adagrad(dis.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d037c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Oracle Sample Loss : 10.9767\n",
      "CPU times: user 4.44 s, sys: 87.2 ms, total: 4.53 s\n",
      "Wall time: 4.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sample from generator and compute oracle NLL\n",
    "oracle_loss = helpers.batchwise_oracle_nll(\n",
    "    gen, \n",
    "    oracle, \n",
    "    POS_NEG_SAMPLES, \n",
    "    BATCH_SIZE, MAX_SEQ_LEN,\n",
    "    start_letter=START_LETTER, \n",
    "    gpu=CUDA,\n",
    ")\n",
    "\n",
    "print('\\nInitial Oracle Sample Loss : %.4f' % oracle_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b637ed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Adversarial Training...\n",
      "\n",
      "--------\n",
      "EPOCH 1\n",
      "--------\n",
      "\n",
      "Adversarial Training Generator : "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ADVERSARIAL TRAINING\n",
    "print('\\nStarting Adversarial Training...')\n",
    "\n",
    "for epoch in range(ADV_TRAIN_EPOCHS):\n",
    "    \n",
    "    print('\\n--------\\nEPOCH %d\\n--------' % (epoch+1))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # TRAIN GENERATOR\n",
    "    print('\\nAdversarial Training Generator : ', end='')\n",
    "    train_generator_PG(gen, gen_optimizer, oracle, dis, \n",
    "                       num_batches = 1,\n",
    "                       POS_NEG_SAMPLES = POS_NEG_SAMPLES,\n",
    "                       BATCH_SIZE = BATCH_SIZE,\n",
    "                       START_LETTER = START_LETTER,\n",
    "                       MAX_SEQ_LEN = MAX_SEQ_LEN,\n",
    "                       CUDA = CUDA,\n",
    "    )\n",
    "\n",
    "    # TRAIN DISCRIMINATOR\n",
    "    print('\\nAdversarial Training Discriminator : ')\n",
    "    train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, \n",
    "                        d_steps = 50,  \n",
    "                        epochs = 3,\n",
    "                        POS_NEG_SAMPLES = POS_NEG_SAMPLES,\n",
    "                        BATCH_SIZE = BATCH_SIZE,\n",
    "                        CUDA = CUDA,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced3347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
